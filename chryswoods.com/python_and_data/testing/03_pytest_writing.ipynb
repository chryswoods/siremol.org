{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest\n",
    "\n",
    "So far we have seen how to run tests using `pytest`. In this section we will learn how to write tests and take advantage of the powerful [`pytest.mark`](https://docs.pytest.org/en/latest/mark.html) to make these tests more useful and informative.\n",
    "\n",
    "## Skipping tests\n",
    "\n",
    "In the previous section we saw that `pytest` could flag tests as `SKIPPED` and report information as to why this was the case. Let's look at one of the test functions to see how this was achieved.\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_mymath.py\n",
    "@pytest.mark.skip(reason=\"Not yet implemented.\")\n",
    "def test_div():\n",
    "    \"\"\" Test the div function. \"\"\"\n",
    "    assert div(9, 2) == approx(4.5)\n",
    "```\n",
    "\n",
    "Here we have written a test for an, as yet, unimplemented function `div` that divides one number by another and returns the result. For a simple function like this the expected output is obvious so it's easy to write a test before the function is even implemented. We are asserting what the _output_ of the function should be, not how it should be _implemented_.\n",
    "\n",
    "Writing tests before implementing functionality is often good practice and is referred [_test-driven development_](https://en.wikipedia.org/wiki/Test-driven_development). Writing tests first can help to better structure your code. Once a test is written you should write the minimum functionality that makes the test pass, then add more tests, and refine.\n",
    "\n",
    "We have marked the test to be skipped by using `@pytest.mark.skip` with a reason given in the parentheses. Don't worry about this funny syntax. It is an example of what's known as a _function decorator_. We are _wrapping_ our test function inside another function called `skip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 1 item                                                              \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_mymath.py s\u001b[36m                                           [100%]\u001b[0m\r\n",
      "=========================== short test summary info ============================\r\n",
      "SKIP [1] mymodule/test/test_mymath.py:30: Not yet implemented.\r\n",
      "\r\n",
      "\u001b[33m\u001b[1m========================== 1 skipped in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest mymodule/test/test_mymath.py::test_div -rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another reason why we might want to skip a test.\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_errors.py\n",
    "@pytest.mark.skipif(sys.platform != 'win32', reason=\"Only runs on windows.\")\n",
    "def test_BSoD():\n",
    "    blueScreenOfDeath()\n",
    "```\n",
    "\n",
    "Here we the test is marked with a _conditional skip_. The test will only be run if the host operating system is Windows. Adding conditional skips like this allows your test suite to be robust and portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 1 item                                                              \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_errors.py s\u001b[36m                                           [100%]\u001b[0m\r\n",
      "=========================== short test summary info ============================\r\n",
      "SKIP [1] mymodule/test/test_errors.py:12: Only runs on Windows.\r\n",
      "\r\n",
      "\u001b[33m\u001b[1m========================== 1 skipped in 0.00 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest mymodule/test/test_errors.py::test_BSoD -rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrizing tests\n",
    "\n",
    "As we have already seen, it is usually desirable to run a test for a range of different input parameters. With `pytest` it is easy to parametrize our test functions.\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_mymath.py\n",
    "@pytest.mark.parametrize(\"x\", [1, 2])\n",
    "@pytest.mark.parametrize(\"y\", [3, 4])\n",
    "def test_mul(x, y):\n",
    "    \"\"\" Test the mul function. \"\"\"\n",
    "    assert mul(x, y) == mul(y, x)\n",
    "```\n",
    "\n",
    "Here the function `test_mul` is parametrized with two parameters, `x` and `y`. By marking the test in this manner it will be executed using all possible parameter pairs `(x, y)`, i.e. `(1, 3), (1, 4), (2, 3), (2, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0 -- /usr/bin/python\r\n",
      "cachedir: .cache\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 4 items                                                             \u001b[0m\u001b[1m\r",
      "collected 4 items                                                              \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_mymath.py::test_mul[3-1] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 25%]\u001b[0m\r\n",
      "mymodule/test/test_mymath.py::test_mul[3-2] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 50%]\u001b[0m\r\n",
      "mymodule/test/test_mymath.py::test_mul[4-1] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 75%]\u001b[0m\r\n",
      "mymodule/test/test_mymath.py::test_mul[4-2] \u001b[32mPASSED\u001b[0m\u001b[36m                       [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 4 passed in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest mymodule/test/test_mymath.py::test_mul -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests can also be parametrized in a different way.\n",
    "\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_mymath.py\n",
    "@pytest.mark.parametrize(\"x, y, expected\",\n",
    "                        [(1, 2, -1),\n",
    "                         (7, 3,  4),\n",
    "                         (21, 58, -37)])\n",
    "def test_sub(x, y, expected):\n",
    "    \"\"\" Test the sub function. \"\"\"\n",
    "    assert sub(x, y) == -sub(y, x) == expected\n",
    "```\n",
    "\n",
    "Here we are passing a list containing different parameter sets, with the names of the parameters matched against the arguments of the test function. Each set of parameters contains the two values to be tested, `x` and `y`, as well as the `expected` outcome of the test. This allows the use of a single `assert` statement in the body of the test function. Can you think why having a single assertion is a good thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0 -- /usr/bin/python\r\n",
      "cachedir: .cache\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 3 items                                                             \u001b[0m\u001b[1m\r",
      "collected 3 items                                                              \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_mymath.py::test_sub[1-2--1] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 33%]\u001b[0m\r\n",
      "mymodule/test/test_mymath.py::test_sub[7-3-4] \u001b[32mPASSED\u001b[0m\u001b[36m                     [ 66%]\u001b[0m\r\n",
      "mymodule/test/test_mymath.py::test_sub[21-58--37] \u001b[32mPASSED\u001b[0m\u001b[36m                 [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 3 passed in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest mymodule/test/test_mymath.py::test_sub -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that it's also important to test for conditions not being met. Here we use an `if` condition inside of the test function to change the `assert` statement depending on the input parameters.\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_mymath.py\n",
    "@pytest.mark.parametrize(\"x, y\",\n",
    "                        [(108, 56),\n",
    "                         (-64, -333),\n",
    "                         (3, 7),\n",
    "                         (74, 15)])\n",
    "def test_greaterThan(x, y):\n",
    "    \"\"\" Test the greaterThan function. \"\"\"\n",
    "    if x > y:\n",
    "        assert greaterThan(x, y)\n",
    "    else:\n",
    "        assert not greaterThan(x, y)\n",
    "```\n",
    "\n",
    "## Expected failures\n",
    "\n",
    "By using marks we can also indicate that we expect a particular test to fail.\n",
    "\n",
    "```python\n",
    "# mymodule/test/test_mymath.py\n",
    "@pytest.mark.xfail(reason=\"Broken code. Working on a fix.\")\n",
    "def test_add():\n",
    "    \"\"\" Test the add function. \"\"\"\n",
    "    assert add(1, 1) ==  2\n",
    "    assert add(1, 2) == add(2, 1) == 3\n",
    "```\n",
    "\n",
    "This is good practice. Rather than hiding tests for our buggy code, we are acknowledging that we are aware of the problem and are working on a fix. The user can query the expected failures and see the reasons for their inclusion. Once bugs have been fixed it is important to keep the tests as part of your codebase. That way you'll know whenever a future change reintroduces a bug that was previously fixed.\n",
    "\n",
    "## Testing exceptions\n",
    "\n",
    "In the previous session you learned how to use exceptions to handle run-time errors in programs. Pytest provides a way of testing your code for known exceptions. For example, suppose we had a function that raises an `IndexError`:\n",
    "\n",
    "```python\n",
    "# mymodule/mymodule.py\n",
    "def indexError():\n",
    "    \"\"\" A function that raises an IndexError. \"\"\"\n",
    "    a = []\n",
    "    a[5]\n",
    "```\n",
    "\n",
    "We could then write a test to check the error is thrown as expected:\n",
    "\n",
    "``` python\n",
    "# mymodule/test/test_errors.py\n",
    "def test_indexError():\n",
    "    with pytest.raises(IndexError):\n",
    "        indexError()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 1 item                                                              \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_errors.py .\u001b[36m                                           [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.00 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest mymodule/test/test_errors.py::test_indexError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom attributes\n",
    "\n",
    "It's possible to mark test functions with any attribute you like. For example:\n",
    "\n",
    "```python\n",
    "@pytest.mark.slow\n",
    "def test_bigSum():\n",
    "    \"\"\" Test the bigSum function. \"\"\"\n",
    "    assert bigSum() == 125000000250000000\n",
    "```\n",
    "\n",
    "Here we have marked the `test_bigSum` function with the attribute `slow` in order to indicate that it takes a while to run. From the command line it is possible to run or skip tests with a particular mark.\n",
    "\n",
    "```bash\n",
    "pytest mymodule -m \"slow\"        # only run the slow tests\n",
    "pytest mymodule -m \"not slow\"    # skip the slow tests\n",
    "```\n",
    "\n",
    "The custom attribute can just be a label, as in this case, or could be your own function decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\n",
      "collected 31 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "mymodule/test/test_mymath.py .\u001b[36m                                           [100%]\u001b[0m\n",
      "\n",
      "\u001b[1m============================= 30 tests deselected ==============================\u001b[0m\n",
      "\u001b[32m\u001b[1m=================== 1 passed, 30 deselected in 2.75 seconds ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -m \"slow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Here you'll be modifying the following files:\n",
    "\n",
    "* `mymodule/mymodule.py`\n",
    "* `mymodule/test/test_mymath.py`\n",
    "* `mymodule/test/test_errors.py`\n",
    "\n",
    "After each exercise, verify that your updated tests work by re-running `pytest`.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Fix the bug in the `add` function in `mymodule.py` and delete the `xfail` mark from `test_add` (since we now expect the test to pass).\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Improve the tests so that they can also handle floating point numbers.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Parametrize the `test_add` function so that it can work with a single assert statement.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "Add a mark to the `tes_mul` function to indicate that it is `critical`. Run `pytest` only for this `critical` test.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Add a test to `test_errors.py` to test the function `keyError` from `mymodule.py`. This functions throws a `KeyError`, i.e. it tries to acess a dictionary using an unknown key.\n",
    "\n",
    "#### Bonus\n",
    "\n",
    "Just because you see tests pass doesn't mean that a piece of software is trustworthy. With a limited number of tests that use a small range of parameters, how can you be sure that the output is correct in all cases? It's also important to remember that tests are themselves just code, so are also prone to errors and bugs. A poor software developer is likely to write poor tests. When writing software it is your job to break things (and then fix them).\n",
    "\n",
    "Can you find the bug in `test_isLucky`?\n",
    "\n",
    "\n",
    "# Further reading\n",
    "\n",
    "Pytest [fixtures](https://docs.pytest.org/en/latest/fixture.html) allow objects to be initalised before test functions are run. This enables objects to be re-used across different tests, which is particularly useful when instantiating objects is complicated or time consuming. Similarly, a fixture can perform clean up once the test functions have finished.\n",
    "\n",
    "Pytest also provides functionality for [mocking](https://docs.pytest.org/en/latest/monkeypatch.html) or [monkey patching](https://en.wikipedia.org/wiki/Monkey_patch) modules and environments. This allows you set up fake objects and environments, allowing your tests to run in situations where they otherwise wouldn't be able to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
